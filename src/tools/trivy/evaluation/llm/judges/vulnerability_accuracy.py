"""Vulnerability accuracy judge for Trivy evaluation.

This judge evaluates the accuracy of CVE vulnerability detection by comparing
detected vulnerabilities against ground truth data, including:
- CVE ID matching and identification
- Severity classification alignment
- Package attribution accuracy
"""

from typing import Any

from .base import BaseJudge, JudgeResult


class VulnerabilityAccuracyJudge(BaseJudge):
    """Judge for evaluating vulnerability identification accuracy.

    This is the most important judge (35% weight) as it validates the core
    functionality of vulnerability detection accuracy.
    """

    @property
    def dimension_name(self) -> str:
        return "vulnerability_accuracy"

    @property
    def weight(self) -> float:
        return 0.35  # 35% of overall score

    def collect_evidence(self) -> dict[str, Any]:
        """Collect evidence about vulnerability accuracy by comparing against ground truth.

        For real-world evaluation mode, also injects synthetic evaluation
        context to help the LLM understand tool baseline capability.
        """
        results = self.load_analysis_results()
        ground_truth = self.load_ground_truth()

        evidence: dict[str, Any] = {
            "evaluation_mode": self.evaluation_mode,
            "summary": {
                "repos_analyzed": len(results),
                "repos_with_ground_truth": 0,
                "total_expected_cves": 0,
                "total_detected_cves": 0,
                "cve_detection_rate": 0.0,
            },
            "per_repo_accuracy": [],
            "cve_matching": [],
            "severity_alignment": [],
            "package_attribution": [],
            "false_positives": [],
            "missed_cves": [],
        }

        total_expected = 0
        total_matched = 0
        total_severity_aligned = 0
        total_package_correct = 0
        severity_checks = 0
        package_checks = 0

        for repo_name, data in results.items():
            vulns = data.get("vulnerabilities", [])
            summary = data.get("summary", {})
            detected_cve_ids = {v.get("id", "") for v in vulns if v.get("id")}

            # Find matching ground truth
            gt = ground_truth.get(repo_name)
            if not gt:
                continue

            evidence["summary"]["repos_with_ground_truth"] += 1

            repo_accuracy = {
                "repo": repo_name,
                "expected_vulns_min": 0,
                "expected_vulns_max": 0,
                "detected_count": len(vulns),
                "known_cves_expected": [],
                "known_cves_detected": [],
                "known_cves_missed": [],
                "within_expected_range": False,
            }

            # Check expected vulnerability counts
            expected_vulns = gt.get("expected_vulnerabilities", {})
            if isinstance(expected_vulns, dict):
                repo_accuracy["expected_vulns_min"] = expected_vulns.get("min", 0)
                repo_accuracy["expected_vulns_max"] = expected_vulns.get("max", 999)
            elif isinstance(expected_vulns, int):
                repo_accuracy["expected_vulns_min"] = expected_vulns
                repo_accuracy["expected_vulns_max"] = expected_vulns

            repo_accuracy["within_expected_range"] = (
                repo_accuracy["expected_vulns_min"]
                <= len(vulns)
                <= repo_accuracy["expected_vulns_max"]
            )

            # Check known CVEs from ground truth
            known_cves = gt.get("known_cves", [])
            repo_accuracy["known_cves_expected"] = known_cves

            for cve_id in known_cves:
                total_expected += 1
                if cve_id in detected_cve_ids:
                    total_matched += 1
                    repo_accuracy["known_cves_detected"].append(cve_id)
                    evidence["cve_matching"].append({
                        "repo": repo_name,
                        "cve_id": cve_id,
                        "status": "detected",
                    })
                else:
                    repo_accuracy["known_cves_missed"].append(cve_id)
                    evidence["cve_matching"].append({
                        "repo": repo_name,
                        "cve_id": cve_id,
                        "status": "missed",
                    })
                    evidence["missed_cves"].append({
                        "repo": repo_name,
                        "cve_id": cve_id,
                        "notes": gt.get("notes", ""),
                    })

            # Check severity accuracy
            expected_critical = gt.get("expected_critical", {})
            expected_high = gt.get("expected_high", {})
            actual_critical = summary.get("critical_count", 0)
            actual_high = summary.get("high_count", 0)

            if isinstance(expected_critical, dict):
                crit_min = expected_critical.get("min", 0)
                crit_max = expected_critical.get("max", 999)
            else:
                crit_min = crit_max = expected_critical

            if isinstance(expected_high, dict):
                high_min = expected_high.get("min", 0)
                high_max = expected_high.get("max", 999)
            else:
                high_min = high_max = expected_high

            severity_aligned = (
                crit_min <= actual_critical <= crit_max
                and high_min <= actual_high <= high_max
            )

            if crit_max > 0 or high_max > 0:
                severity_checks += 1
                if severity_aligned:
                    total_severity_aligned += 1

            evidence["severity_alignment"].append({
                "repo": repo_name,
                "critical_expected": f"{crit_min}-{crit_max}",
                "critical_actual": actual_critical,
                "high_expected": f"{high_min}-{high_max}",
                "high_actual": actual_high,
                "aligned": severity_aligned,
            })

            # Check package attribution
            required_packages = gt.get("required_packages", [])
            if required_packages:
                detected_packages = {v.get("package", "") for v in vulns}
                for pkg in required_packages:
                    package_checks += 1
                    if pkg in detected_packages:
                        total_package_correct += 1
                        evidence["package_attribution"].append({
                            "repo": repo_name,
                            "package": pkg,
                            "status": "detected",
                        })
                    else:
                        evidence["package_attribution"].append({
                            "repo": repo_name,
                            "package": pkg,
                            "status": "missed",
                        })

            evidence["per_repo_accuracy"].append(repo_accuracy)

        # Calculate summary statistics
        evidence["summary"]["total_expected_cves"] = total_expected
        evidence["summary"]["total_detected_cves"] = total_matched
        if total_expected > 0:
            evidence["summary"]["cve_detection_rate"] = round(
                total_matched / total_expected * 100, 1
            )

        evidence["summary"]["severity_alignment_rate"] = (
            round(total_severity_aligned / severity_checks * 100, 1)
            if severity_checks > 0
            else 100.0
        )

        evidence["summary"]["package_attribution_rate"] = (
            round(total_package_correct / package_checks * 100, 1)
            if package_checks > 0
            else 100.0
        )

        # Inject synthetic context for real-world evaluation
        if self.evaluation_mode == "real_world":
            synthetic_context = self.load_synthetic_evaluation_context()
            if synthetic_context:
                evidence["synthetic_baseline"] = synthetic_context
                evidence["interpretation_guidance"] = self.get_interpretation_guidance(
                    synthetic_context
                )

        return evidence

    def run_ground_truth_assertions(self) -> tuple[bool, list[str]]:
        """Run ground truth assertions before LLM evaluation.

        Key assertions:
        1. At least one ground truth file should match analysis results
        2. Known CVEs should have high detection rate (>50%)
        3. Severity ranges should be within expected bounds

        Returns:
            Tuple of (all_passed, list of failure messages)
        """
        failures = []

        evidence = self.collect_evidence()

        # Assertion 1: Must have ground truth matches
        if evidence["summary"]["repos_with_ground_truth"] == 0:
            failures.append("No analysis results matched any ground truth files")

        # Assertion 2: CVE detection rate should be reasonable
        detection_rate = evidence["summary"]["cve_detection_rate"]
        if evidence["summary"]["total_expected_cves"] > 0 and detection_rate < 50:
            failures.append(
                f"CVE detection rate too low: {detection_rate}% (expected >= 50%)"
            )

        # Assertion 3: Check for repos within expected vulnerability ranges
        repos_in_range = sum(
            1 for r in evidence["per_repo_accuracy"] if r["within_expected_range"]
        )
        total_repos = len(evidence["per_repo_accuracy"])
        if total_repos > 0 and repos_in_range / total_repos < 0.5:
            failures.append(
                f"Only {repos_in_range}/{total_repos} repos have vulnerability "
                f"counts within expected ranges"
            )

        return len(failures) == 0, failures

    def evaluate(self) -> JudgeResult:
        """Run the evaluation pipeline with ground truth assertions.

        1. Collects evidence from analysis results and ground truth
        2. Runs ground truth assertions (caps score at 2 if failed)
        3. Builds prompt from template
        4. Invokes Claude for evaluation
        5. Parses response into JudgeResult
        """
        # Run ground truth assertions first
        gt_passed, gt_failures = self.run_ground_truth_assertions()

        # Collect evidence
        evidence = self.collect_evidence()

        # Add assertion results to evidence
        evidence["ground_truth_assertions"] = {
            "passed": gt_passed,
            "failures": gt_failures,
        }

        # Build prompt
        prompt = self.build_prompt(evidence)

        # Invoke Claude
        response = self.invoke_claude(prompt)

        # Parse response
        result = self.parse_response(response)

        # Cap score if ground truth assertions failed
        if not gt_passed:
            original_score = result.score
            result.score = min(result.score, 2)
            if result.score != original_score:
                result.reasoning = (
                    f"[Score capped from {original_score} to {result.score} due to "
                    f"ground truth assertion failures: {gt_failures}] "
                    + result.reasoning
                )

        return result
