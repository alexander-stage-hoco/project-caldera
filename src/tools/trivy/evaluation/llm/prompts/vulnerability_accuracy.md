# Vulnerability Accuracy Evaluation

You are evaluating the accuracy of Trivy's CVE vulnerability detection capabilities for a due diligence code analysis tool.

## Context

Trivy is a security scanner that detects CVE vulnerabilities in software dependencies. For due diligence purposes, accurate vulnerability detection is critical for:
- Identifying security liabilities in acquisition targets
- Assessing supply chain risk
- Determining remediation costs

## Evidence

The following evidence shows vulnerability detection results compared against ground truth data:

{{ evidence }}

## Evaluation Criteria

Score 1-5 based on three sub-dimensions:

1. **CVE Identification (40%)**: Are known CVEs correctly identified?
   - Check `cve_matching` to see which known CVEs were detected vs missed
   - Check `cve_detection_rate` for overall detection percentage
   - High detection rate (>90%) = excellent, 70-90% = good, 50-70% = acceptable, <50% = poor

2. **Severity Classification (30%)**: Do severity levels match NVD classification?
   - Check `severity_alignment` to see if CRITICAL/HIGH counts are within expected ranges
   - Check `severity_alignment_rate` for overall alignment percentage
   - All repos aligned = excellent, >75% aligned = good, 50-75% = acceptable, <50% = poor

3. **Package Attribution (30%)**: Are vulnerabilities linked to correct packages?
   - Check `package_attribution` to see if required packages were detected
   - Check `package_attribution_rate` for overall attribution accuracy
   - 100% = excellent, >90% = good, 70-90% = acceptable, <70% = poor

### Scoring Rubric

| Score | Criteria |
|-------|----------|
| 5 | All known CVEs detected (>95%), all severities aligned, all packages attributed |
| 4 | Most known CVEs detected (85-95%), minor severity mismatches, >90% packages |
| 3 | Majority of CVEs detected (70-85%), some alignment issues, >70% packages |
| 2 | Significant CVE gaps (50-70%), multiple alignment issues, <70% packages |
| 1 | Major CVE detection failures (<50%), unreliable severity or attribution |

### Ground Truth Assertions

If `ground_truth_assertions.passed` is `false`, the score should be capped at 2 regardless of other factors. The failures explain why assertions failed.

## Response Format

Respond with ONLY a JSON object (no markdown code fences):

{
  "score": <1-5>,
  "confidence": <0.0-1.0>,
  "reasoning": "<2-3 sentences explaining the score based on evidence>",
  "evidence_cited": ["<specific evidence points from the data above>"],
  "recommendations": ["<actionable improvements if score < 5>"],
  "sub_scores": {
    "cve_identification": <1-5>,
    "severity_classification": <1-5>,
    "package_attribution": <1-5>
  }
}
