# Vulnerability Accuracy Evaluation

You are evaluating the accuracy of Trivy's CVE vulnerability detection capabilities for a due diligence code analysis tool.

## Evaluation Context

{{ interpretation_guidance }}

### Synthetic Repo Validation Results
{{ synthetic_baseline }}

### Evaluation Mode
{{ evaluation_mode }}

**Important**: When evaluation_mode is "real_world":
- Low vulnerability counts are NOT automatically failures
- Judge output quality: schema compliance, severity classification, package attribution
- Judge detection quality: Are the vulnerabilities that WERE detected accurate and properly classified?
- Consider: A tool that finds 0 vulnerabilities in a repo with up-to-date dependencies deserves a high score

## Context

Trivy is a security scanner that detects CVE vulnerabilities in software dependencies. For due diligence purposes, accurate vulnerability detection is critical for:
- Identifying security liabilities in acquisition targets
- Assessing supply chain risk
- Determining remediation costs

## Evidence

The following evidence shows vulnerability detection results compared against ground truth data:

{{ evidence }}

## Evaluation Criteria

Score 1-5 based on three sub-dimensions:

1. **CVE Identification (40%)**: Are known CVEs correctly identified?
   - Check `cve_matching` to see which known CVEs were detected vs missed
   - Check `cve_detection_rate` for overall detection percentage
   - High detection rate (>90%) = excellent, 70-90% = good, 50-70% = acceptable, <50% = poor

2. **Severity Classification (30%)**: Do severity levels match NVD classification?
   - Check `severity_alignment` to see if CRITICAL/HIGH counts are within expected ranges
   - Check `severity_alignment_rate` for overall alignment percentage
   - All repos aligned = excellent, >75% aligned = good, 50-75% = acceptable, <50% = poor

3. **Package Attribution (30%)**: Are vulnerabilities linked to correct packages?
   - Check `package_attribution` to see if required packages were detected
   - Check `package_attribution_rate` for overall attribution accuracy
   - 100% = excellent, >90% = good, 70-90% = acceptable, <70% = poor

### Scoring Rubric

#### For Synthetic Repos (with ground truth):
| Score | Criteria |
|-------|----------|
| 5 | All known CVEs detected (>95%), all severities aligned, all packages attributed |
| 4 | Most known CVEs detected (85-95%), minor severity mismatches, >90% packages |
| 3 | Majority of CVEs detected (70-85%), some alignment issues, >70% packages |
| 2 | Significant CVE gaps (50-70%), multiple alignment issues, <70% packages |
| 1 | Major CVE detection failures (<50%), unreliable severity or attribution |

#### For Real-World Repos (when synthetic_baseline shows validated tool):
| Score | Criteria |
|-------|----------|
| 5 | Output schema compliant, any vulnerabilities accurately classified, complete metadata |
| 4 | Minor schema issues but vulnerabilities accurate, good severity mapping |
| 3 | Schema issues OR questionable vulnerability classification |
| 2 | Multiple schema issues AND questionable classifications |
| 1 | Broken output, missing required fields, obvious false positives |

**Key principle**: Do NOT penalize for low vulnerability counts on real-world repos when the tool is validated (synthetic_score >= 0.9). A well-maintained codebase with clean dependencies deserves a high score.

### Ground Truth Assertions

If `ground_truth_assertions.passed` is `false`, the score should be capped at 2 regardless of other factors. The failures explain why assertions failed.

## Response Format

Respond with ONLY a JSON object (no markdown code fences):

{
  "score": <1-5>,
  "confidence": <0.0-1.0>,
  "reasoning": "<2-3 sentences explaining the score based on evidence>",
  "evidence_cited": ["<specific evidence points from the data above>"],
  "recommendations": ["<actionable improvements if score < 5>"],
  "sub_scores": {
    "cve_identification": <1-5>,
    "severity_classification": <1-5>,
    "package_attribution": <1-5>
  }
}
