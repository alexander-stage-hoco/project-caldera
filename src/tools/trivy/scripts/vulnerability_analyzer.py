#!/usr/bin/env python3
"""Vulnerability analyzer using Trivy for CVE detection and SBOM generation."""
from __future__ import annotations

import argparse
import json
import logging
import subprocess
import tempfile
import time
from collections import defaultdict
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from registry_clients import (
    fetch_freshness_batch,
    get_registry_fetcher,
    calculate_version_delta,
    calculate_days_since,
)

logger = logging.getLogger(__name__)


@dataclass
class VulnerabilityFinding:
    """A single vulnerability finding from Trivy."""

    id: str
    severity: str
    package: str
    installed_version: str
    fixed_version: str
    title: str
    description: str
    cvss_score: float
    cvss_vector: str
    published_date: str
    age_days: int
    fix_available: bool
    target: str
    target_type: str


@dataclass
class PackageFreshness:
    """Package version freshness info for outdatedness tracking."""

    package: str
    package_type: str
    installed_version: str
    latest_version: str
    major_versions_behind: int
    minor_versions_behind: int
    patch_versions_behind: int
    is_outdated: bool
    days_since_latest: int
    has_vulnerability: bool
    vulnerability_count: int


@dataclass
class MisconfigurationFinding:
    """An IaC misconfiguration finding from Trivy."""

    id: str
    severity: str
    title: str
    description: str
    resolution: str
    target: str
    target_type: str
    start_line: int
    end_line: int


@dataclass
class TargetSummary:
    """Summary of vulnerabilities for a single target (lockfile)."""

    path: str
    type: str
    vulnerability_count: int
    critical_count: int
    high_count: int
    medium_count: int
    low_count: int
    dependency_count: int


@dataclass
class DirectoryMetrics:
    """Metrics for a directory."""

    vulnerability_count: int
    critical_count: int
    high_count: int
    medium_count: int
    low_count: int
    target_count: int


@dataclass
class SBOMSummary:
    """SBOM/package inventory summary."""

    format: str = "trivy-json"
    total_packages: int = 0
    vulnerable_packages: int = 0
    clean_packages: int = 0


@dataclass
class VulnerabilityAnalysis:
    """Complete vulnerability analysis output."""

    schema_version: str = "1.0.0"
    repo_name: str = ""
    repo_path: str = ""
    generated_at: str = ""
    tool: str = "trivy"
    tool_version: str = ""
    scan_time_ms: float = 0.0

    # Summary
    total_vulnerabilities: int = 0
    critical_count: int = 0
    high_count: int = 0
    medium_count: int = 0
    low_count: int = 0
    unknown_count: int = 0
    fix_available_count: int = 0
    fix_available_pct: float = 0.0
    dependency_count: int = 0
    oldest_cve_days: int = 0
    targets_scanned: int = 0

    # IaC misconfigurations
    iac_total_count: int = 0
    iac_critical_count: int = 0
    iac_high_count: int = 0

    # SBOM/Package inventory
    sbom_total_packages: int = 0
    sbom_vulnerable_packages: int = 0
    sbom_clean_packages: int = 0

    # Freshness/outdatedness tracking
    freshness_checked: bool = False
    outdated_count: int = 0
    outdated_pct: float = 0.0
    major_versions_behind_total: int = 0
    avg_days_behind: float = 0.0
    package_freshness: list[PackageFreshness] = field(default_factory=list)

    # Detailed findings
    vulnerabilities: list[VulnerabilityFinding] = field(default_factory=list)
    misconfigurations: list[MisconfigurationFinding] = field(default_factory=list)

    # Target summaries
    targets: list[TargetSummary] = field(default_factory=list)

    # Directory rollups
    directories: dict[str, Any] = field(default_factory=dict)


def get_trivy_version(trivy_path: Path) -> str:
    """Get Trivy version string."""
    result = subprocess.run(
        [str(trivy_path), "version", "--format", "json"],
        capture_output=True,
        text=True,
    )
    if result.returncode == 0:
        try:
            version_info = json.loads(result.stdout)
            return version_info.get("Version", "unknown")
        except json.JSONDecodeError:
            pass
    return "unknown"


def run_trivy(trivy_path: Path, repo_path: Path) -> tuple[dict, float]:
    """Run Trivy on a repository and return parsed JSON output."""
    with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as f:
        report_path = Path(f.name)

    start_time = time.time()

    # Run trivy fs with vulnerability and config scanners
    # Use --list-all-pkgs to capture all packages (not just vulnerable ones) for SBOM
    result = subprocess.run(
        [
            str(trivy_path),
            "fs",
            "--format",
            "json",
            "--scanners",
            "vuln,misconfig",
            "--list-all-pkgs",
            "--output",
            str(report_path),
            str(repo_path),
        ],
        capture_output=True,
        text=True,
    )

    elapsed_ms = (time.time() - start_time) * 1000

    # Trivy returns exit code 0 for success (even with findings)
    # or non-zero for errors
    if result.returncode not in (0, 1):
        # Check if it's just a warning
        if "WARN" not in result.stderr and "no supported file" not in result.stderr.lower():
            print(f"Warning: trivy returned {result.returncode}: {result.stderr}")

    # Parse results
    if report_path.exists() and report_path.stat().st_size > 0:
        try:
            raw_output = json.loads(report_path.read_text())
        except json.JSONDecodeError:
            raw_output = {}
    else:
        raw_output = {}

    report_path.unlink(missing_ok=True)

    return raw_output, elapsed_ms


def run_trivy_sbom(
    trivy_path: Path,
    repo_path: Path,
    output_path: Path,
    sbom_format: str = "cyclonedx",
) -> bool:
    """Generate SBOM using Trivy.

    Args:
        trivy_path: Path to trivy binary
        repo_path: Path to repository to scan
        output_path: Path to save SBOM output
        sbom_format: SBOM format - 'cyclonedx', 'spdx', or 'spdx-json'

    Returns:
        True if SBOM was generated successfully
    """
    valid_formats = {"cyclonedx", "spdx", "spdx-json"}
    if sbom_format not in valid_formats:
        print(f"Invalid SBOM format: {sbom_format}. Must be one of {valid_formats}")
        return False

    result = subprocess.run(
        [
            str(trivy_path),
            "fs",
            "--format",
            sbom_format,
            "--output",
            str(output_path),
            str(repo_path),
        ],
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        print(f"SBOM generation failed: {result.stderr}")
        return False

    return output_path.exists() and output_path.stat().st_size > 0


def calculate_age_days(published_date: str) -> int:
    """Calculate days since CVE was published."""
    if not published_date:
        return 0
    try:
        # Handle various date formats
        if "T" in published_date:
            pub_date = datetime.fromisoformat(published_date.replace("Z", "+00:00"))
        else:
            pub_date = datetime.strptime(published_date, "%Y-%m-%d").replace(
                tzinfo=timezone.utc
            )
        now = datetime.now(timezone.utc)
        return max(0, (now - pub_date).days)
    except (ValueError, TypeError):
        return 0


def parse_trivy_output(raw_output: dict) -> tuple[list[VulnerabilityFinding], list[MisconfigurationFinding], list[TargetSummary], int]:
    """Parse Trivy JSON output into structured findings.

    Returns:
        Tuple of (vulnerabilities, misconfigurations, targets, total_packages)
    """
    vulnerabilities = []
    misconfigurations = []
    targets = []
    total_packages = 0

    results = raw_output.get("Results", [])

    for result in results:
        target = result.get("Target", "unknown")
        target_type = result.get("Type", "unknown")

        # Parse vulnerabilities
        vulns = result.get("Vulnerabilities") or []
        target_vuln_count = 0
        target_critical = 0
        target_high = 0
        target_medium = 0
        target_low = 0
        dep_count = set()

        for vuln in vulns:
            severity = vuln.get("Severity", "UNKNOWN").upper()

            # Get CVSS score
            cvss_score = 0.0
            cvss_vector = ""
            cvss = vuln.get("CVSS", {})
            for vendor in ["nvd", "redhat", "ghsa"]:
                if vendor in cvss:
                    cvss_score = cvss[vendor].get("V3Score", 0.0) or cvss[vendor].get(
                        "V2Score", 0.0
                    )
                    cvss_vector = cvss[vendor].get("V3Vector", "") or cvss[vendor].get(
                        "V2Vector", ""
                    )
                    break

            published_date = vuln.get("PublishedDate", "")
            fixed_version = vuln.get("FixedVersion", "")

            finding = VulnerabilityFinding(
                id=vuln.get("VulnerabilityID", "unknown"),
                severity=severity,
                package=vuln.get("PkgName", "unknown"),
                installed_version=vuln.get("InstalledVersion", ""),
                fixed_version=fixed_version,
                title=vuln.get("Title", ""),
                description=vuln.get("Description", "")[:500],  # Truncate
                cvss_score=cvss_score,
                cvss_vector=cvss_vector,
                published_date=published_date,
                age_days=calculate_age_days(published_date),
                fix_available=bool(fixed_version),
                target=target,
                target_type=target_type,
            )
            vulnerabilities.append(finding)
            target_vuln_count += 1
            dep_count.add(vuln.get("PkgName", ""))

            if severity == "CRITICAL":
                target_critical += 1
            elif severity == "HIGH":
                target_high += 1
            elif severity == "MEDIUM":
                target_medium += 1
            elif severity == "LOW":
                target_low += 1

        # Parse misconfigurations
        misconfigs = result.get("Misconfigurations") or []
        for misconfig in misconfigs:
            finding = MisconfigurationFinding(
                id=misconfig.get("ID", "unknown"),
                severity=misconfig.get("Severity", "UNKNOWN").upper(),
                title=misconfig.get("Title", ""),
                description=misconfig.get("Description", "")[:500],
                resolution=misconfig.get("Resolution", ""),
                target=target,
                target_type=result.get("Class", "config"),
                start_line=misconfig.get("CauseMetadata", {}).get("StartLine", 0),
                end_line=misconfig.get("CauseMetadata", {}).get("EndLine", 0),
            )
            misconfigurations.append(finding)

        # Count all packages (from --list-all-pkgs flag)
        packages = result.get("Packages") or []
        total_packages += len(packages)

        # Create target summary if there are vulnerabilities
        if target_vuln_count > 0 or len(misconfigs) > 0:
            targets.append(
                TargetSummary(
                    path=target,
                    type=target_type,
                    vulnerability_count=target_vuln_count,
                    critical_count=target_critical,
                    high_count=target_high,
                    medium_count=target_medium,
                    low_count=target_low,
                    dependency_count=len(dep_count),
                )
            )

    return vulnerabilities, misconfigurations, targets, total_packages


def fetch_freshness_data(
    raw_output: dict,
    vulnerabilities: list[VulnerabilityFinding],
) -> list[PackageFreshness]:
    """Fetch freshness info for all packages from registries.

    Args:
        raw_output: Raw Trivy JSON output containing Results with Packages
        vulnerabilities: List of vulnerability findings to cross-reference

    Returns:
        List of PackageFreshness objects for all packages that could be checked
    """
    # Build lookup for vulnerable packages
    vuln_by_pkg: dict[str, list[VulnerabilityFinding]] = defaultdict(list)
    for vuln in vulnerabilities:
        vuln_by_pkg[vuln.package].append(vuln)

    freshness_list: list[PackageFreshness] = []
    results = raw_output.get("Results", [])

    for result in results:
        pkg_type = result.get("Type", "unknown")
        packages = result.get("Packages") or []

        if not packages:
            continue

        # Check if we have a registry fetcher for this package type
        if not get_registry_fetcher(pkg_type):
            logger.debug(f"No registry fetcher for package type: {pkg_type}")
            continue

        # Batch fetch freshness info
        logger.info(f"Fetching freshness for {len(packages)} {pkg_type} packages...")
        freshness_info = fetch_freshness_batch(packages, pkg_type)

        # Build PackageFreshness objects
        for pkg in packages:
            name = pkg.get("Name", "")
            installed = pkg.get("Version", "unknown")

            if name not in freshness_info:
                continue

            info = freshness_info[name]
            vulns = vuln_by_pkg.get(name, [])

            freshness_list.append(
                PackageFreshness(
                    package=name,
                    package_type=pkg_type,
                    installed_version=installed,
                    latest_version=info["latest_version"],
                    major_versions_behind=info.get("major_behind", 0),
                    minor_versions_behind=info.get("minor_behind", 0),
                    patch_versions_behind=info.get("patch_behind", 0),
                    is_outdated=info.get("is_outdated", False),
                    days_since_latest=info.get("days_since_latest", 0),
                    has_vulnerability=len(vulns) > 0,
                    vulnerability_count=len(vulns),
                )
            )

    return freshness_list


def compute_directory_rollups(
    vulnerabilities: list[VulnerabilityFinding],
) -> dict[str, Any]:
    """Compute direct and recursive directory rollups."""
    dir_direct: dict[str, list[VulnerabilityFinding]] = defaultdict(list)
    dir_recursive: dict[str, list[VulnerabilityFinding]] = defaultdict(list)
    dir_targets_direct: dict[str, set[str]] = defaultdict(set)
    dir_targets_recursive: dict[str, set[str]] = defaultdict(set)

    for vuln in vulnerabilities:
        target_path = Path(vuln.target)
        parent = str(target_path.parent) if target_path.parent != target_path else "."

        # Direct: vulnerabilities in this directory's lockfiles
        dir_direct[parent].append(vuln)
        dir_targets_direct[parent].add(vuln.target)

        # Recursive: add to all ancestors
        parts = target_path.parts
        for i in range(len(parts)):
            if i == 0:
                ancestor = parts[0] if parts[0] != target_path.name else "."
            else:
                ancestor = str(Path(*parts[:i]))
            dir_recursive[ancestor].append(vuln)
            dir_targets_recursive[ancestor].add(vuln.target)

        # Root always gets everything
        dir_recursive["."].append(vuln)
        dir_targets_recursive["."].add(vuln.target)

    def build_metrics(vulns: list[VulnerabilityFinding], targets: set[str]) -> DirectoryMetrics:
        """Build metrics from vulnerability list."""
        critical = sum(1 for v in vulns if v.severity == "CRITICAL")
        high = sum(1 for v in vulns if v.severity == "HIGH")
        medium = sum(1 for v in vulns if v.severity == "MEDIUM")
        low = sum(1 for v in vulns if v.severity == "LOW")
        return DirectoryMetrics(
            vulnerability_count=len(vulns),
            critical_count=critical,
            high_count=high,
            medium_count=medium,
            low_count=low,
            target_count=len(targets),
        )

    # Build directory entries
    all_dirs = set(dir_direct.keys()) | set(dir_recursive.keys())
    directories_list = []

    for dir_path in sorted(all_dirs):
        direct_vulns = dir_direct.get(dir_path, [])
        recursive_vulns = dir_recursive.get(dir_path, [])
        direct_targets = dir_targets_direct.get(dir_path, set())
        recursive_targets = dir_targets_recursive.get(dir_path, set())

        directories_list.append(
            {
                "path": dir_path,
                "direct": asdict(build_metrics(direct_vulns, direct_targets)),
                "recursive": asdict(build_metrics(recursive_vulns, recursive_targets)),
            }
        )

    return {"directory_count": len(directories_list), "directories": directories_list}


def analyze_repository(
    trivy_path: Path,
    repo_path: Path,
    check_freshness: bool = False,
) -> VulnerabilityAnalysis:
    """Analyze a repository for vulnerabilities and optionally check freshness.

    Args:
        trivy_path: Path to trivy binary
        repo_path: Path to repository to analyze
        check_freshness: If True, query package registries for latest versions

    Returns:
        VulnerabilityAnalysis with all findings and optionally freshness data
    """
    repo_name = repo_path.name

    # Run Trivy
    raw_output, scan_time = run_trivy(trivy_path, repo_path)

    # Parse output
    vulnerabilities, misconfigurations, targets, total_packages = parse_trivy_output(raw_output)

    # Build analysis
    analysis = VulnerabilityAnalysis(
        repo_name=repo_name,
        repo_path=str(repo_path.resolve()),
        generated_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        tool_version=get_trivy_version(trivy_path),
        scan_time_ms=scan_time,
    )

    # Count by severity
    for vuln in vulnerabilities:
        if vuln.severity == "CRITICAL":
            analysis.critical_count += 1
        elif vuln.severity == "HIGH":
            analysis.high_count += 1
        elif vuln.severity == "MEDIUM":
            analysis.medium_count += 1
        elif vuln.severity == "LOW":
            analysis.low_count += 1
        else:
            analysis.unknown_count += 1

        if vuln.fix_available:
            analysis.fix_available_count += 1

    analysis.total_vulnerabilities = len(vulnerabilities)
    analysis.fix_available_pct = (
        (analysis.fix_available_count / analysis.total_vulnerabilities * 100)
        if analysis.total_vulnerabilities > 0
        else 0.0
    )

    # Oldest CVE
    if vulnerabilities:
        analysis.oldest_cve_days = max(v.age_days for v in vulnerabilities)

    # Targets scanned
    analysis.targets_scanned = len(targets)

    # IaC misconfigurations
    analysis.iac_total_count = len(misconfigurations)
    analysis.iac_critical_count = sum(
        1 for m in misconfigurations if m.severity == "CRITICAL"
    )
    analysis.iac_high_count = sum(1 for m in misconfigurations if m.severity == "HIGH")

    # SBOM/Package inventory
    unique_vulnerable_packages = set(v.package for v in vulnerabilities)
    analysis.sbom_total_packages = total_packages
    analysis.sbom_vulnerable_packages = len(unique_vulnerable_packages)
    analysis.sbom_clean_packages = max(0, total_packages - len(unique_vulnerable_packages))

    # Update dependency_count to use total packages (all, not just vulnerable)
    analysis.dependency_count = total_packages if total_packages > 0 else len(unique_vulnerable_packages)

    # Store detailed findings
    analysis.vulnerabilities = vulnerabilities
    analysis.misconfigurations = misconfigurations
    analysis.targets = targets

    # Compute directory rollups
    analysis.directories = compute_directory_rollups(vulnerabilities)

    # Freshness checking (optional, requires network)
    if check_freshness:
        analysis.freshness_checked = True
        analysis.package_freshness = fetch_freshness_data(raw_output, vulnerabilities)

        if analysis.package_freshness:
            analysis.outdated_count = sum(
                1 for p in analysis.package_freshness if p.is_outdated
            )
            analysis.outdated_pct = (
                analysis.outdated_count / len(analysis.package_freshness) * 100
            )
            analysis.major_versions_behind_total = sum(
                p.major_versions_behind for p in analysis.package_freshness
            )
            days_behind = [p.days_since_latest for p in analysis.package_freshness if p.days_since_latest > 0]
            analysis.avg_days_behind = sum(days_behind) / len(days_behind) if days_behind else 0.0

    return analysis


def analysis_to_dict(analysis: VulnerabilityAnalysis) -> dict:
    """Convert analysis to dictionary for JSON serialization."""
    # Build results object with tool-specific data
    results = {
        "tool": analysis.tool,
        "tool_version": analysis.tool_version,
        "scan_time_ms": analysis.scan_time_ms,
        "summary": {
            "total_vulnerabilities": analysis.total_vulnerabilities,
            "critical_count": analysis.critical_count,
            "high_count": analysis.high_count,
            "medium_count": analysis.medium_count,
            "low_count": analysis.low_count,
            "unknown_count": analysis.unknown_count,
            "fix_available_count": analysis.fix_available_count,
            "fix_available_pct": round(analysis.fix_available_pct, 2),
            "dependency_count": analysis.dependency_count,
            "oldest_cve_days": analysis.oldest_cve_days,
            "targets_scanned": analysis.targets_scanned,
        },
        "vulnerabilities": [asdict(v) for v in analysis.vulnerabilities],
        "targets": [asdict(t) for t in analysis.targets],
        "directories": analysis.directories,
        "iac_misconfigurations": {
            "total_count": analysis.iac_total_count,
            "critical_count": analysis.iac_critical_count,
            "high_count": analysis.iac_high_count,
            "misconfigurations": [asdict(m) for m in analysis.misconfigurations],
        },
        "sbom": {
            "format": "trivy-json",
            "total_packages": analysis.sbom_total_packages,
            "vulnerable_packages": analysis.sbom_vulnerable_packages,
            "clean_packages": analysis.sbom_clean_packages,
        },
    }

    # Add freshness section if checked
    if analysis.freshness_checked:
        results["freshness"] = {
            "checked": True,
            "total_packages": len(analysis.package_freshness),
            "outdated_count": analysis.outdated_count,
            "outdated_pct": round(analysis.outdated_pct, 2),
            "major_versions_behind_total": analysis.major_versions_behind_total,
            "avg_days_behind": round(analysis.avg_days_behind, 1),
            "packages": [
                {
                    "package": p.package,
                    "package_type": p.package_type,
                    "installed_version": p.installed_version,
                    "latest_version": p.latest_version,
                    "major_versions_behind": p.major_versions_behind,
                    "minor_versions_behind": p.minor_versions_behind,
                    "patch_versions_behind": p.patch_versions_behind,
                    "is_outdated": p.is_outdated,
                    "days_since_latest": p.days_since_latest,
                    "has_vulnerability": p.has_vulnerability,
                    "vulnerability_count": p.vulnerability_count,
                }
                for p in analysis.package_freshness
            ],
        }

    # Build final output with standardized root fields
    return {
        "schema_version": analysis.schema_version,
        "generated_at": analysis.generated_at,
        "repo_name": analysis.repo_name,
        "repo_path": analysis.repo_path,
        "results": results,
    }


def analyze_all_repos(
    trivy_path: Path,
    repos_dir: Path,
    output_dir: Path,
    generate_sbom: bool = False,
    sbom_format: str = "cyclonedx",
    check_freshness: bool = False,
) -> dict[str, VulnerabilityAnalysis]:
    """Analyze all repositories in a directory."""
    results = {}

    for repo_path in sorted(repos_dir.iterdir()):
        if not repo_path.is_dir():
            continue

        print(f"Analyzing: {repo_path.name}")
        analysis = analyze_repository(trivy_path, repo_path, check_freshness=check_freshness)
        results[repo_path.name] = analysis

        # Save individual result
        output_path = output_dir / f"{repo_path.name}.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(json.dumps(analysis_to_dict(analysis), indent=2))

        print(f"  Vulnerabilities: {analysis.total_vulnerabilities}")
        print(f"  Critical: {analysis.critical_count}, High: {analysis.high_count}")
        print(f"  IaC issues: {analysis.iac_total_count}")
        print(f"  Output: {output_path}")

        # Generate SBOM if requested
        if generate_sbom:
            sbom_ext = "json" if sbom_format == "cyclonedx" else sbom_format.replace("-", ".")
            sbom_path = output_dir / f"{repo_path.name}.sbom.{sbom_ext}"
            if run_trivy_sbom(trivy_path, repo_path, sbom_path, sbom_format):
                print(f"  SBOM: {sbom_path}")

    return results


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Analyze repositories for vulnerabilities using Trivy"
    )
    parser.add_argument(
        "path",
        type=Path,
        help="Repository or directory of repositories to analyze",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        default=Path("output/runs"),
        help="Output path (file.json for single repo, or directory for multi-repo)",
    )
    parser.add_argument(
        "--trivy",
        type=Path,
        default=None,
        help="Path to trivy binary",
    )
    parser.add_argument(
        "--sbom",
        action="store_true",
        help="Generate CycloneDX SBOM alongside vulnerability analysis",
    )
    parser.add_argument(
        "--sbom-format",
        choices=["cyclonedx", "spdx", "spdx-json"],
        default="cyclonedx",
        help="SBOM format (default: cyclonedx)",
    )
    parser.add_argument(
        "--repo-name",
        type=str,
        default=None,
        help="Repository name for output file naming (used by orchestrator)",
    )
    parser.add_argument(
        "--check-freshness",
        action="store_true",
        help="Query package registries for latest versions (requires network)",
    )

    args = parser.parse_args()

    # Find trivy
    if args.trivy:
        trivy_path = args.trivy
    else:
        # Try common locations
        script_dir = Path(__file__).parent
        trivy_path = script_dir.parent / "bin" / "trivy"

    if not trivy_path.exists():
        raise FileNotFoundError(f"trivy not found at {trivy_path}")

    print(f"Using trivy: {trivy_path}")
    print(f"Trivy version: {get_trivy_version(trivy_path)}")
    print()

    # Determine if path is single repo or directory of repos
    # Check for package.json, requirements.txt, etc. to detect if it's a repo
    manifest_files = [
        "package.json", "requirements.txt", "go.mod", "Gemfile", "pom.xml",
        "Dockerfile", "main.tf", "Cargo.toml", "build.gradle", "CMakeLists.txt",
        "packages.lock.json", "packages.config",  # NuGet
        "gradle.lockfile",  # Gradle
    ]
    # Also check for C# projects (*.csproj, *.sln, *.fsproj)
    has_csharp = any(args.path.glob("*.csproj")) or any(args.path.glob("*.sln")) or any(args.path.glob("*.fsproj"))
    # Check for Kubernetes manifests (*.yaml with apiVersion)
    has_k8s = any(
        "apiVersion" in f.read_text()[:200] if f.is_file() else False
        for f in list(args.path.glob("*.yaml"))[:5] + list(args.path.glob("*.yml"))[:5]
    )
    # Check for CloudFormation templates
    has_cfn = any(
        "AWSTemplateFormatVersion" in f.read_text()[:500] if f.is_file() else False
        for f in list(args.path.glob("*.yaml"))[:5] + list(args.path.glob("*.json"))[:5]
    )
    is_single_repo = has_csharp or has_k8s or has_cfn or any((args.path / f).exists() for f in manifest_files)

    if is_single_repo:
        # Single repository
        print(f"Analyzing single repository: {args.path}")
        if args.check_freshness:
            print("Freshness checking enabled (will query package registries)")
        analysis = analyze_repository(trivy_path, args.path, check_freshness=args.check_freshness)

        # Determine output path: if --output ends with .json, use it directly
        # Otherwise, construct filename from repo-name or path name
        if str(args.output).endswith(".json"):
            output_path = args.output
        else:
            output_name = args.repo_name if args.repo_name else args.path.name
            output_path = args.output / f"{output_name}.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(json.dumps(analysis_to_dict(analysis), indent=2))

        print(f"Vulnerabilities found: {analysis.total_vulnerabilities}")
        print(f"  Critical: {analysis.critical_count}")
        print(f"  High: {analysis.high_count}")
        if analysis.freshness_checked:
            print(f"  Outdated packages: {analysis.outdated_count}/{len(analysis.package_freshness)} ({analysis.outdated_pct:.1f}%)")
        print(f"Output: {output_path}")

        # Generate SBOM if requested
        if args.sbom:
            sbom_ext = "json" if args.sbom_format == "cyclonedx" else args.sbom_format.replace("-", ".")
            sbom_path = args.output / f"{args.path.name}.sbom.{sbom_ext}"
            if run_trivy_sbom(trivy_path, args.path, sbom_path, args.sbom_format):
                print(f"SBOM ({args.sbom_format}): {sbom_path}")
            else:
                print(f"Warning: Failed to generate SBOM")
    else:
        # Directory of repositories
        print(f"Analyzing all repositories in: {args.path}")
        if args.check_freshness:
            print("Freshness checking enabled (will query package registries)")
        print()

        results = analyze_all_repos(
            trivy_path, args.path, args.output, args.sbom, args.sbom_format,
            check_freshness=args.check_freshness
        )

        print()
        print("=" * 60)
        print("Summary")
        print("=" * 60)
        total_vulns = sum(r.total_vulnerabilities for r in results.values())
        total_critical = sum(r.critical_count for r in results.values())
        total_iac = sum(r.iac_total_count for r in results.values())
        print(f"Repositories analyzed: {len(results)}")
        print(f"Total vulnerabilities: {total_vulns}")
        print(f"Total critical: {total_critical}")
        print(f"Total IaC issues: {total_iac}")


if __name__ == "__main__":
    main()
