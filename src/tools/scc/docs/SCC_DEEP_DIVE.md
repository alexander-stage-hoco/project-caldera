# scc Deep Dive - Complete Feature Analysis

## Executive Summary

**scc** (Sloc Cloc and Code) v3.6.0 is a high-performance code counter written in Go.

| Aspect | Value |
|--------|-------|
| Languages | **351** supported |
| Output Formats | **12** formats |
| Performance | **5ms** for 63 files |
| License | MIT / Unlicense |

**We're currently using ~20% of scc's capabilities.**

---

## 1. OUTPUT FORMATS (12 Total)

### Currently Used
```bash
./bin/scc -f json              # Language-level aggregates
```

### Available But Unused

| Format | Flag | Use Case |
|--------|------|----------|
| **json2** | `-f json2` | Includes COCOMO in JSON structure |
| **csv** | `-f csv` | Spreadsheet import |
| **sql** | `-f sql` | Direct SQLite/DuckDB import |
| **sql-insert** | `-f sql-insert` | Append to existing tables |
| **openmetrics** | `-f openmetrics` | Prometheus/Grafana monitoring |
| **html** | `-f html` | Standalone HTML report |
| **cloc-yaml** | `-f cloc-yaml` | CLOC-compatible format |

### Multi-Format Output (Single Pass)
```bash
./bin/scc --format-multi "json:file.json,csv:file.csv,sql:file.sql"
```
Generates all formats in one run.

---

## 2. PER-FILE ANALYSIS (`--by-file`)

### Current: Language-Level Only
```json
{"Name":"Python","Lines":892,"Code":765,"Complexity":56,"Count":9}
```

### Available: Per-File Detail
```bash
./bin/scc --by-file -f json
```
```json
{
  "Location": "src/module/file.py",
  "Filename": "file.py",
  "Extension": "py",
  "Lines": 47,
  "Code": 33,
  "Comment": 5,
  "Blank": 9,
  "Bytes": 1170,
  "Complexity": 14,
  "Uloc": 37,
  "Minified": false,
  "Generated": false,
  "Binary": false,
  "Hash": null,
  "WeightedComplexity": 0
}
```

### Per-File Fields Available

| Field | Type | Description |
|-------|------|-------------|
| `Location` | string | Full path |
| `Filename` | string | Base filename |
| `Extension` | string | File extension |
| `Language` | string | Detected language |
| `Lines` | int | Total lines |
| `Code` | int | Code lines |
| `Comment` | int | Comment lines |
| `Blank` | int | Blank lines |
| `Bytes` | int | File size |
| `Complexity` | int | Cyclomatic complexity |
| `Uloc` | int | Unique lines (with `--uloc`) |
| `Minified` | bool | Minified file flag |
| `Generated` | bool | Auto-generated flag |
| `Binary` | bool | Binary file flag |
| `Hash` | string | File hash (for dedup) |

**DD Platform Use Case**: Feed directly into Layer 1 collectors for `lines_of_code`, `complexity_score`, `comment_ratio` per file.

---

## 3. CODE DUPLICATION METRICS

### ULOC (Unique Lines of Code)
```bash
./bin/scc --uloc
```
```
Total Lines:              7,666
Unique Lines of Code:     4,203
```

### DRYness Percentage
```bash
./bin/scc --dryness
```
```
DRYness %: 0.55
```
**Interpretation**: 55% of lines are unique, 45% are duplicated.

### Per-Language ULOC
```
Language     Lines    ULOC    DRY%
Python         892     605    67.8%
C#           1,046     539    51.5%
Java         1,286     710    55.2%
Go           1,127     678    60.2%
```

**DD Platform Use Case**: Code duplication indicator for tech debt assessment.

---

## 4. FILE QUALITY FLAGS

### Minified File Detection (`--min`)
```bash
./bin/scc --min --by-file -f json
```
- Default threshold: 255 bytes average per line
- Configurable: `--min-gen-line-length 255`
- Result: `"Minified": true` in JSON

### Generated File Detection (`--gen`)
```bash
./bin/scc --gen --by-file -f json
```
- Default markers: `"do not edit"`, `"<auto-generated />"`
- Configurable: `--generated-markers "DO NOT MODIFY,auto-generated"`
- Result: `"Generated": true` in JSON

### Exclude These Files (`--no-min --no-gen`)
```bash
./bin/scc --no-min-gen   # Exclude both minified and generated
```

**DD Platform Use Case**: Exclude noise from analysis, flag non-human code.

---

## 5. COCOMO COST ESTIMATION

### Built-in Project Types
| Type | Formula | Example (5,885 LOC) |
|------|---------|---------------------|
| **organic** | 2.4 × KLOC^1.05 | $173,711 / 7.07 mo / 2.18 people |
| **semi-detached** | 3.0 × KLOC^1.12 | $245,822 / 7.36 mo / 2.97 people |
| **embedded** | 3.6 × KLOC^1.20 | $339,924 / 7.44 mo / 4.06 people |

### Basic Configuration
```bash
./bin/scc --cocomo-project-type embedded
./bin/scc --avg-wage 100000          # Default: $56,286
./bin/scc --overhead 3.0             # Default: 2.4
./bin/scc --eaf 1.5                  # Effort adjustment factor
```

### Custom COCOMO Coefficients (Full Parameterization)

scc supports fully custom COCOMO models via:
```bash
./bin/scc --cocomo-project-type "custom,a,b,c,d"
```

Where:
- `a` = Effort multiplier (default organic: 2.4, semi-detached: 3.0, embedded: 3.6)
- `b` = Effort exponent (default organic: 1.05, semi-detached: 1.12, embedded: 1.20)
- `c` = Schedule multiplier (default: 2.5)
- `d` = Schedule exponent (default: 0.38)

**COCOMO Formulas:**
```
Effort (person-months) = a × (KLOC)^b × EAF
Schedule (months) = c × (Effort)^d
Cost = Effort × (avg_wage / 12) × overhead
People = Effort / Schedule
```

### Example Custom Model
```bash
# Enterprise software with higher complexity
./bin/scc --cocomo-project-type "custom,3.0,1.12,2.5,0.35" \
          --avg-wage 120000 \
          --overhead 2.8 \
          --eaf 1.2

# Startup pace (lower overhead)
./bin/scc --cocomo-project-type "custom,2.4,1.05,2.5,0.38" \
          --avg-wage 150000 \
          --overhead 1.8 \
          --eaf 0.9
```

### Reference: Industry-Standard Coefficients

| Project Type | a | b | c | d | Description |
|--------------|---|---|---|---|-------------|
| Organic | 2.4 | 1.05 | 2.5 | 0.38 | Small teams, familiar tech |
| Semi-detached | 3.0 | 1.12 | 2.5 | 0.35 | Medium teams, mixed experience |
| Embedded | 3.6 | 1.20 | 2.5 | 0.32 | Tight constraints, high reliability |

### JSON2 Format (Includes COCOMO)
```bash
./bin/scc -f json2
```
```json
{
  "languageSummary": [...],
  "estimatedCost": 173711.06,
  "estimatedScheduleMonths": 7.07,
  "estimatedPeople": 2.18
}
```

### SLOCCount Format
```bash
./bin/scc --sloccount-format
```
```
Total Physical SLOC = 5,885
Development Effort = 1.29 Person-Years (15.43 Person-Months)
Schedule Estimate = 0.59 Years (7.07 Months)
Average Developers = 2.18
Total Cost = $173,712
```

**DD Platform Use Case**: Effort estimation evidence for due diligence with customizable parameters.

---

## 6. DIRECTORY TREE ANALYSIS

scc does not have native directory-level rollup, but provides two approaches:

### Approach 1: Post-Process Raw JSON Output

```bash
./bin/scc --by-file -f json > raw_scc_output.json
```

Then aggregate in Python:
```python
import json
from collections import defaultdict
from pathlib import Path

with open('raw_scc_output.json') as f:
    data = json.load(f)

dir_stats = defaultdict(lambda: {
    'files': 0, 'code': 0, 'complexity': 0, 'languages': set()
})

for lang in data:
    for f in lang.get('Files', []):
        path = Path(f['Location'])
        # Accumulate to each parent directory
        for i in range(1, len(path.parts)):
            dir_path = '/'.join(path.parts[:i])
            dir_stats[dir_path]['files'] += 1
            dir_stats[dir_path]['code'] += f.get('Code', 0)
            dir_stats[dir_path]['complexity'] += f.get('Complexity', 0)
            dir_stats[dir_path]['languages'].add(lang['Name'])

# Print tree
for path in sorted(dir_stats.keys()):
    stats = dir_stats[path]
    indent = '  ' * (path.count('/'))
    print(f"{indent}{path.split('/')[-1]}/")
    print(f"{indent}  -> {stats['files']} files, {stats['code']} LOC, "
          f"{stats['complexity']} complexity")
```

### Example Directory Tree Output
```
eval-repos/
  -> 63 files, 5885 LOC, 1027 complexity
  synthetic/
    -> 63 files, 5885 LOC, 1027 complexity
    python/
      -> 9 files, 765 LOC, 56 complexity
      edge_cases/
        -> 5 files, 53 LOC, 24 complexity
    csharp/
      -> 9 files, 1046 LOC, 181 complexity
    java/
      -> 9 files, 1286 LOC, 203 complexity
```

### Approach 2: SQL Output with GROUP BY

scc SQL output includes `File_dirname`:
```bash
./bin/scc -f sql --sql-project "my-project" > scc.sql
```

Schema includes:
```sql
create table t (
  Project text,
  Language text,
  File text,
  File_dirname text,    -- Directory path
  File_basename text,   -- Filename only
  nCode integer,
  nComplexity integer,
  ...
);
```

Query for directory rollups:
```sql
SELECT
    File_dirname,
    COUNT(*) as files,
    SUM(nCode) as total_code,
    SUM(nComplexity) as total_complexity,
    COUNT(DISTINCT Language) as languages
FROM t
GROUP BY File_dirname
ORDER BY total_code DESC;
```

### Use Case: Architecture Analysis

Directory-level metrics enable:
- **Module Complexity**: Identify high-complexity directories
- **Layered Analysis**: Compare src/ vs tests/ vs docs/
- **Ownership Mapping**: Map directories to teams
- **Dependency Analysis**: Correlate with namespace structure

---

## 7. FILE FILTERING

### Include Only Specific Extensions
```bash
./bin/scc -i py,js,ts    # Only Python, JS, TypeScript
```

### Exclude Extensions
```bash
./bin/scc -x java,cs     # Exclude Java and C#
```

### Exclude Directories
```bash
./bin/scc --exclude-dir node_modules,vendor,dist
```
Default excludes: `.git`, `.hg`, `.svn`

### Regex Exclusion
```bash
./bin/scc -M ".*test.*"        # Exclude test directories
./bin/scc -M ".*generated.*"   # Exclude generated directories
```

### Exclude Large Files
```bash
./bin/scc --no-large
./bin/scc --large-byte-count 500000    # 500KB max
./bin/scc --large-line-count 10000     # 10K lines max
```

### Exclude Default Lock Files
Already excluded: `package-lock.json`, `Cargo.lock`, `yarn.lock`, etc.

---

## 8. SQL OUTPUT (Direct DB Import)

### Full SQL with Schema
```bash
./bin/scc -f sql --sql-project "my-project"
```
```sql
create table metadata (
  timestamp text, Project text, elapsed_s real,
  estimated_cost real, estimated_schedule_months real,
  estimated_people real
);

create table t (
  Project text, Language text, File text,
  File_dirname text, File_basename text,
  nByte integer, nBlank integer, nComment integer,
  nCode integer, nComplexity integer, nUloc integer
);

insert into t values('my-project', 'Python',
  'src/module/file.py', 'src/module/', 'file.py',
  1170, 9, 5, 33, 14, 37);
```

### SQL Insert Only (Existing Tables)
```bash
./bin/scc -f sql-insert --sql-project "my-project"
```

### DuckDB Integration
```bash
./bin/scc -f sql --sql-project "analysis-run-001" > scc.sql
duckdb mydb.duckdb < scc.sql
```

**DD Platform Use Case**: Direct Layer 1 import, skip JSON transform.

---

## 9. CHARACTER METRICS (`-m`)

```bash
./bin/scc -m
```
```
Language    Files  Lines  MaxLine  MeanLine
Python          9    892      107        28
C#              9  1,046      129        31
Java            9  1,286      120        28
```

**Use Case**: Detect files with excessively long lines (code style issues).

---

## 10. DUPLICATE DETECTION (`--no-duplicates`)

```bash
./bin/scc --no-duplicates
```
```
With duplicates:    63 files, 7,666 lines
Without duplicates: 56 files, 7,643 lines
```
Removes identical files from statistics.

---

## 11. PROMETHEUS/OPENMETRICS OUTPUT

```bash
./bin/scc -f openmetrics
```
```
# TYPE scc_files gauge
scc_files{language="Python"} 9
scc_code{language="Python"} 765
scc_complexity{language="Python"} 56
scc_bytes{language="Python"} 26520
```

**Use Case**: Monitoring dashboard, Grafana integration.

---

## 12. LANGUAGE REMAPPING

### Remap Unknown Extensions
```bash
./bin/scc --remap-unknown "-*- Python -*-":Python
```

### Remap by Content
```bash
./bin/scc --remap-all "#!/usr/bin/env python":Python
```

### Count Extension as Different Language
```bash
./bin/scc --count-as jsx:JavaScript,mjs:JavaScript
```

---

## 13. PERFORMANCE TUNING

### Parallelization Controls
```bash
./bin/scc --directory-walker-job-workers 16    # Default: 8
./bin/scc --file-process-job-workers 16        # Default: 8
./bin/scc --file-gc-count 20000                # GC every N files
```

### Benchmark
```
63 files:   5ms (default workers)
63 files:   5ms (16 workers - same, I/O bound)
```

---

## GAP ANALYSIS: What We're Missing

| Feature | Current | Available | Priority |
|---------|:-------:|:---------:|:--------:|
| Per-file metrics | ✗ | ✓ | **HIGH** |
| Per-file complexity | ✗ | ✓ | **HIGH** |
| ULOC per file | ✗ | ✓ | **HIGH** |
| Directory rollups | ✗ | ✓ | **HIGH** |
| DRYness % | ✗ | ✓ | **MEDIUM** |
| Minified detection | ✗ | ✓ | **MEDIUM** |
| Generated detection | ✗ | ✓ | **MEDIUM** |
| COCOMO estimates | ✗ | ✓ | **MEDIUM** |
| Custom COCOMO params | ✗ | ✓ | **MEDIUM** |
| SQL direct import | ✗ | ✓ | **LOW** |
| OpenMetrics | ✗ | ✓ | **LOW** |
| Multi-format output | ✗ | ✓ | **LOW** |

---

## RECOMMENDED COMMANDS

### Current (Minimal)
```bash
./bin/scc eval-repos/synthetic -f json
```

### Recommended (Full Features)
```bash
./bin/scc eval-repos/synthetic \
  --by-file \
  --uloc \
  --dryness \
  --min \
  --gen \
  -f json2
```

### With Custom COCOMO
```bash
./bin/scc eval-repos/synthetic \
  --by-file \
  --uloc \
  --dryness \
  --min \
  --gen \
  --cocomo-project-type "custom,3.0,1.12,2.5,0.35" \
  --avg-wage 120000 \
  --overhead 2.8 \
  -f json2
```

This gives us:
- Per-file LOC, complexity, ULOC
- DRYness percentage
- Minified/generated file flags
- Customized COCOMO estimates in JSON

---

## INTEGRATION WITH DD PLATFORM

### Layer 1 (Collectors) Integration Points

| scc Field | DD Metric | Entity |
|-----------|-----------|--------|
| `Code` | `lines_of_code` | File |
| `Complexity` | `complexity_score` | File |
| `Comment` | `comment_ratio` (computed) | File |
| `Uloc` | `unique_loc` | File |
| `Minified` | `is_minified` | File |
| `Generated` | `is_generated` | File |
| `DRYness` | `dryness_pct` | Project |
| `estimatedCost` | `cocomo_cost` | Project |
| `File_dirname` | directory rollup | Namespace |

### Recommended Collector Updates

1. **ComplexityCollector**: Use `--by-file` output for per-file metrics
2. **New SccMetricsCollector**: Extract ULOC, DRYness, minified/generated flags
3. **CocomoCollector**: Extract cost estimates with configurable parameters
4. **DirectoryMetricsCollector**: Post-process for namespace-level rollups
